{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2",
        "outputId": "3efac7f2-b42f-470c-a4c1-b18b2cd9f267"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from timm.models.vision_transformer import vit_base_patch16_224\n",
        "from torch import nn\n",
        "!pip install -q lightly[timm]\n",
        "from lightly.models import utils\n",
        "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM\n",
        "from lightly.transforms import MAETransform\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import pyplot as plt\n",
        "from lightly.data import LightlyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NienSmKtyxnj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NienSmKtyxnj",
        "outputId": "6f8dff09-edd2-4cad-cb05-d4b493cc2cf6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-ts53uVyzhs",
      "metadata": {
        "id": "A-ts53uVyzhs"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/My Drive/au_opg/train.zip' -d '/content/au_opg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "class MAE(nn.Module):\n",
        "    def __init__(self, vit):\n",
        "        super().__init__()\n",
        "\n",
        "        decoder_dim = 512\n",
        "        self.mask_ratio = 0.75\n",
        "        self.patch_size = vit.patch_embed.patch_size[0]\n",
        "\n",
        "        self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
        "        self.sequence_length = self.backbone.sequence_length\n",
        "        self.decoder = MAEDecoderTIMM(\n",
        "            num_patches=vit.patch_embed.num_patches,\n",
        "            patch_size=self.patch_size,\n",
        "            embed_dim=vit.embed_dim,\n",
        "            decoder_embed_dim=decoder_dim,\n",
        "            decoder_depth=1,\n",
        "            decoder_num_heads=16,\n",
        "            mlp_ratio=4.0,\n",
        "            proj_drop_rate=0.0,\n",
        "            attn_drop_rate=0.0,\n",
        "        )\n",
        "\n",
        "    def forward_encoder(self, images, idx_keep=None):\n",
        "        return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
        "\n",
        "    def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
        "        # build decoder input\n",
        "        batch_size = x_encoded.shape[0]\n",
        "        x_decode = self.decoder.embed(x_encoded)\n",
        "        x_masked = utils.repeat_token(\n",
        "            self.decoder.mask_token, (batch_size, self.sequence_length)\n",
        "        )\n",
        "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
        "\n",
        "        # decoder forward pass\n",
        "        x_decoded = self.decoder.decode(x_masked)\n",
        "\n",
        "        # predict pixel values for masked tokens\n",
        "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
        "        x_pred = self.decoder.predict(x_pred)\n",
        "        return x_pred\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # Generate random token masks\n",
        "        idx_keep, idx_mask = utils.random_token_mask(\n",
        "            size=(batch_size, self.sequence_length-1),\n",
        "            mask_ratio=self.mask_ratio,\n",
        "            device=images.device,\n",
        "        )\n",
        "\n",
        "        # Encode the unmasked patches\n",
        "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
        "        # print('0', x_encoded.shape)\n",
        "\n",
        "        # Decode to predict the masked patches\n",
        "        x_pred = self.forward_decoder(x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask)\n",
        "        # Get the original image patches\n",
        "        patches = utils.patchify(images, self.patch_size)\n",
        "\n",
        "        reconstructed_patches = torch.zeros(size=(batch_size, patches.shape[1], patches.shape[-1])).to('cuda')\n",
        "        masked = reconstructed_patches.clone().to('cuda')\n",
        "        # Place the original patches in the unmasked positions\n",
        "        reconstructed_patches = utils.set_at_index(reconstructed_patches, idx_keep, utils.get_at_index(patches, idx_keep))\n",
        "        masked = utils.set_at_index(masked, idx_keep, utils.get_at_index(patches, idx_keep))\n",
        "\n",
        "        # Place the predicted patches in the masked positions\n",
        "        reconstructed_patches = utils.set_at_index(\n",
        "            reconstructed_patches, idx_mask, x_pred\n",
        "        )\n",
        "        # print('3', reconstructed_patches.shape)\n",
        "\n",
        "        # Reconstruct the full image from the patches\n",
        "        reconstructed_image = utils.unpatchify(reconstructed_patches, patch_size=self.patch_size)\n",
        "        masked = utils.unpatchify(masked, patch_size=self.patch_size)\n",
        "\n",
        "        # Get the ground truth for the masked patches\n",
        "        target = utils.get_at_index(patches, idx_mask)\n",
        "\n",
        "        return x_pred, target, reconstructed_image, masked\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "outputs": [],
      "source": [
        "vit = vit_base_patch16_224()\n",
        "model = MAE(vit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.autograd.detect_anomaly()\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "id": "7",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# transform = MAETransform(min_scale=1, normalize={\"mean\": [0.5,0.5,0.5], \"std\":[0.5,0.5,0.5]})\n",
        "# print(transform.transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85dd21e2",
      "metadata": {
        "id": "85dd21e2"
      },
      "outputs": [],
      "source": [
        "def min_max_normalize(tensor):\n",
        "    min_val = tensor.min()\n",
        "    max_val = tensor.max()\n",
        "    return (tensor - min_val) / (max_val - min_val)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    min_max_normalize,\n",
        "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]),\n",
        "])\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "dataset = LightlyDataset(\"/content/au_opg/train\", transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10",
        "outputId": "9d2e03d7-550c-4afc-d842-793269cab319"
      },
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "print(len(dataloader), 'batches')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tzM5RPRS9Ego",
      "metadata": {
        "id": "tzM5RPRS9Ego"
      },
      "source": [
        "Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mpFya1Q57fpg",
      "metadata": {
        "id": "mpFya1Q57fpg"
      },
      "outputs": [],
      "source": [
        "first_batch = next(iter(dataloader))\n",
        "model = model.to(device)\n",
        "output = model(first_batch[0].to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ax3kHrRg9F9W",
      "metadata": {
        "id": "Ax3kHrRg9F9W"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0048836",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "d0048836",
        "outputId": "85036abd-0f1a-4166-b693-853361245a5c"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,3,figsize=(10,10))\n",
        "# pil_rev = transforms.Compose([\n",
        "#     transforms.Normalize([-1,-1,-1], [2,2,2]),\n",
        "#     transforms.ToPILImage()\n",
        "# ])\n",
        "pil_rev = transforms.Compose([transforms.Normalize(mean=[0., 0., 0.],\n",
        "                                                    std=[1/0.229, 1/0.224, 1/0.225]),\n",
        "                               transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n",
        "                                                    std=[1., 1., 1.]),\n",
        "                               ])\n",
        "img = 6\n",
        "ax[0].imshow(pil_rev(output[2][img].cpu().detach()).permute(1, 2, 0))\n",
        "ax[1].imshow(pil_rev(output[3][img].cpu().detach()).permute(1, 2, 0))\n",
        "ax[2].imshow(pil_rev(first_batch[0][img].cpu().detach()).permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "12",
        "outputId": "e44be223-f8c0-48ed-cd63-22b58a011b45"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Training\")\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        images = batch[0].to(device)\n",
        "        # print(views.shape)\n",
        "        images = images.to(device)  # views contains only a single view\n",
        "        predictions, targets, _, _= model(images)\n",
        "        loss = criterion(predictions, targets)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tMGTEtl28kLE",
      "metadata": {
        "id": "tMGTEtl28kLE"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YK0kdsf48lLs",
      "metadata": {
        "id": "YK0kdsf48lLs"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "invTrans = transforms.Compose([transforms.Normalize(mean=[0., 0., 0.],\n",
        "                                                    std=[1/0.229, 1/0.224, 1/0.225]),\n",
        "                               transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n",
        "                                                    std=[1., 1., 1.]),\n",
        "                               ])\n",
        "model.eval()\n",
        "original_images, generated_images = [], []\n",
        "num_images = 5\n",
        "model = model.to(device)\n",
        "train_iter = iter(dataloader)\n",
        "images = next(train_iter)[0][0]\n",
        "for i in range(num_images):\n",
        "    x = invTrans(images[i])\n",
        "    original_images.append(x.permute(1, 2, 0).to('cpu').numpy())\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "    yHat = model(x)\n",
        "    print(yHat[0].shape)\n",
        "    yHat = invTrans(yHat[2].squeeze(0))\n",
        "    print(yHat.shape)\n",
        "    generated_images.append(yHat.permute(1, 2, 0).detach().to('cpu').numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O834e8Un9AsO",
      "metadata": {
        "id": "O834e8Un9AsO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "first_channel, second_channel, third_channel = original_images[0][:, :, 0], original_images[0][:, :, 1], original_images[0][:, :, 2]\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(first_channel, cmap='gray')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(second_channel, cmap='gray')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(third_channel, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YVfATxH_9Cir",
      "metadata": {
        "id": "YVfATxH_9Cir"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(2, num_images, i + 1)\n",
        "    plt.imshow(original_images[i])\n",
        "    plt.axis('off')\n",
        "    plt.subplot(2, num_images, i + num_images + 1)\n",
        "    plt.imshow(generated_images[i])\n",
        "    plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "dl-310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
